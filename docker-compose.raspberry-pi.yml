# Docker Compose configuration optimized for Raspberry Pi :-)
# Includes all fixes: chat feature, PDF generation, and resource management

version: '3.8'

services:
  gpt-researcher:
    build:
      context: .
      dockerfile: Dockerfile
      # Use buildx for ARM64 architecture
      platforms:
        - linux/arm64
    container_name: gpt-researcher-pi
    ports:
      - "8000:8000"
    environment:
      # Required API Keys - set these in .env file
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      
      # Optional: Local LLM support (Ollama)
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - FAST_LLM_MODEL=${FAST_LLM_MODEL:-gpt-4o-mini}
      - SMART_LLM_MODEL=${SMART_LLM_MODEL:-gpt-4o}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      
      # Research configuration
      - MAX_SEARCH_RESULTS_PER_QUERY=${MAX_SEARCH_RESULTS_PER_QUERY:-5}
      - MAX_ITERATIONS=${MAX_ITERATIONS:-3}
      - RETRIEVER=${RETRIEVER:-tavily}
      
      # Performance tuning for Raspberry Pi
      - WORKERS=1
      - HOST=0.0.0.0
      - PORT=8000
    
    volumes:
      # Persist research outputs
      - ./outputs:/usr/src/app/outputs
      # Persist logs
      - ./logs:/usr/src/app/logs
      # Mount .env file
      - ./.env:/usr/src/app/.env:ro
    
    # Resource limits for Raspberry Pi (adjust based on your Pi model)
    deploy:
      resources:
        limits:
          # Pi 4 8GB or Pi 5: use 4G
          # Pi 4 4GB: use 2G
          # Pi 3: use 1G
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    # Restart policy
    restart: unless-stopped
    
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Optional: Ollama for local LLM (recommended for privacy)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-pi
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
    restart: unless-stopped
    profiles:
      - local-llm

volumes:
  ollama-data:
    driver: local

# To use this file:
# 1. Copy .env.example to .env and add your API keys
# 2. Build and run:
#    docker-compose -f docker-compose.raspberry-pi.yml up -d
# 
# To include local LLM (Ollama):
#    docker-compose -f docker-compose.raspberry-pi.yml --profile local-llm up -d
#
# To pull a model in Ollama:
#    docker exec -it ollama-pi ollama pull llama3.2:1b

